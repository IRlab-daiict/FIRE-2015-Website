<!--#include virtual="head.html" -->
<div class="right">
<h2>Abstracts</h2>

<br/>

<a name="Jaap">
<div style="font-size:120%;font-weight:bold;padding-bottom:1em;">Finding Pages on the Unarchived Web</div>
</a>
<div style="font-size:110%;padding-bottom:1em;">
  <a href="http://staff.science.uva.nl/~kamps/">
  Jaap Kamps, University of Amsterdam, The Netherlands.
  </a>
</div>
<p>
Web archives preserve the fast changing Web, yet are highly incomplete due
to crawling restrictions, crawling depth and frequency, or restrictive
selection policies—most of the Web is unarchived and therefore lost to
posterity. We propose an approach to recover significant parts of the
unarchived Web, by reconstructing descriptions of these pages based on
links and anchors in the set of crawled pages, and experiment with this
approach on the Dutch Web archive.  Our main findings are threefold. First,
the crawled Web contains evidence of a remarkable number of unarchived
pages and websites, potentially dramatically increasing the coverage of the
Web archive. Second, the link and anchor descriptions have a highly skewed
distribution: popular pages such as home pages have more terms, but the
richness tapers off quickly. Third, the succinct representation is
generally rich enough to uniquely identify pages on the unarchived Web: in
a known-item search setting we can retrieve these pages within the first
ranks on average.
</p>
<br>
<!--p>
<b>About the Speaker:</b>
Douglas Oard is a Professor at the University of Maryland, College Park, with joint appointments in the 
College of Information Studies and the Institute for Advanced Computer Studies. He is a General Co-Chair 
for NTCIR-11 and he has served as a track or task coordinator at TREC and CLEF, and FIRE. Additional information 
is available at <a href="http://terpconnect.umd.edu/~oard/."> http://terpconnect.umd.edu/~oard/ </a>

</p-->

<br> <hr> <br>

<a name="Muller">
<div style="font-size:120%;font-weight:bold;padding-bottom:1em;">Medical Tasks as part of the CLEF campaign</div>
</a>
<div style="font-size:110%;padding-bottom:1em;">
  <a href="http://www.henningmueller.info/">
  Henning Müller, University Hospitals and University of Geneva, Switzerland.
  </a>
</div>
<p>
Both ImageCLEF and CLEFeHealth have been run as part of the Cross-Language
Evaluation Forum for several years and highlight the importance of medical
information retrieval and multilingual access to it. Medical data sets have
some constraints that are different from other data, particularly if
patient data are concerned. Also the tasks need to be developed in
collaboration with health professionals to be useful.
The presentation will show an overview of the tasks run in the past and
some of the results, including the specific parts of medical data.


</p>
<br>
<!--p>
<b>About the Speaker:</b>
Jacques Savoy is a Professor at the University of Neuchatel (Switzerland).
His research interests cover mainly natural language processing and particularly information
retrieval for languages other than English (European, Asian, and Indian) as well as multilingual
and cross-lingual information retrieval. His current research interests are related to text
clustering and categorization as well as authorship attribution.
</p-->

<br> <hr> <br>

<a name="Kevyn">
<div style="font-size:120%;font-weight:bold;padding-bottom:1em;">Examining the Limits of Crowdsourcing for Relevance Assessment</div>
</a>
<div style="font-size:110%;padding-bottom:1em;">
  <a href="http://ir.shef.ac.uk/cloughie/">Paul Clough, The University of Sheffield, UK.</a>
</div>
<p>
Evaluation is instrumental in the development and management of effective
information retrieval systems and ensuring high levels of user
satisfaction. Using crowdsourcing as part of this process has been shown to
be viable. What is less well understood are the limits of crowdsourcing for
evaluation, particularly for domain specific search. I will present results
comparing relevance assessments gathered using crowdsourcing with those
gathered from a domain expert for evaluating different search engines in a
large government archive. While crowdsourced judgments rank the tested
search engines in the same order as expert judgments, crowdsourced workers
appear unable to distinguish different levels of highly accurate search
results in a way that expert assessors can. The nature of this limitation
in crowd sourced workers for this experiment is examined and the viability
of crowdsourcing for evaluating search in specialist settings is discussed.

</p>
<br>
<!--p>
<b>About the Speaker:</b>
Dr. Kevyn Collins-Thompson is an Associate Professor at the University of Michigan, with appointments in the School of Information and Dept. of EE/CS.
 His research explores advances in information systems that can reliably and automatically adapt to users and their 
 information needs in different contexts, especially to help human learning. His research on personalization has been 
 applied to real-world systems ranging from intelligent tutoring systems to Web search engines. Kevyn has also pioneered 
 techniques for modeling reading difficulty, and understanding how people learn new words. He received his Ph.D. in Computer Science 
 from Carnegie Mellon University, where he was a member of the Language Technologies Institute. Before joining the University of Michigan 
 he spent five years as a Researcher at Microsoft Research. His research has been recognized by paper awards that include an ACM SIGIR'13 
 Best Student-led Paper award (for his work on intrinsic diversity with Karthik Raman and Paul Bennett) and 
 an ACM SIGIR'12 Best Paper Honorable Mention (for his work with Lidan Wang and Paul Bennett on risk-sensitive ranking).
</p-->

<br> <hr> <br>

<a name="Moens">
<div style="font-size:120%;font-weight:bold;padding-bottom:1em;">Assessing Performances across Evaluation Cycles: The Example of CLEF 15th
Birthday</div>
</a>
<div style="font-size:110%;padding-bottom:1em;">
  <a href="http://ims.dei.unipd.it/websites/archive/ims2009/members/ferro/">
    Nicola Ferro, University of Padova, Italy.
  </a>
</div>
<p>
Since 2014 marks the 15th birthday of CLEF, we have conducted a
longitudinal study to assess the impact of CLEF evaluation cycles for
multilingual ad-hoc retrieval. Monolingual retrieval shows a positive
trend, even if the performance increase is not always steady from year to
year; bilingual retrieval demonstrates higher improvements in recent years,
probably due to the better linguistic resources now available; and,
multilingual retrieval exhibit constant improvement and performances
comparable to bilingual (and, sometimes, even monolingual) ones. We will
also discuss the methodology adopted to compare results across evaluation
cycles and test collections and highlight some of its limitations.

</p>

<br> <hr> <br>


</ul>
</i>


</div>
<!--#include virtual="foot.html" -->
